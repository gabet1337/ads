\documentclass[a4paper,oneside,article,11pt]{memoir}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}

% This font looks so good.
\usepackage[sc]{mathpazo}

% Typesetting pseudo-code
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
% Code comments like [CLRS]
\renewcommand{\algorithmiccomment}[1]{\makebox[5cm][l]{$\triangleright$ \textit{#1}}}
\usepackage{framed,graphicx,xcolor}
\usepackage[font={small,it}]{caption}
\usepackage{listings}
\usepackage{units}

% Relative references
\usepackage{varioref}

\usepackage{hyperref}

\bibliographystyle{plain}

\title{Advanced Data Structures \\ Project 1 - Fibonacci heaps}
\author{Peter Gabrielsen 20114179 \\
Christoffer Hansen 20114637}
\newcounter{qcounter}
\begin{document}

\begin{titlingpage}
\clearpage

\maketitle
\thispagestyle{empty}

\begin{abstract}
ABSTRACT HERE
\\
\\
\\
The code can be found at: \\\url{CODE HERE}.
\end{abstract}
\end{titlingpage}

\pagebreak

\tableofcontents

\pagebreak

\chapter{Introduction}
INTRO

\chapter{Implementation}
We will in this section describe and discuss the design choices of our implementations of the binary heap and the Fibonacci heap.
\subsection{Binary heap}

\subsection{Fibonacci heap}

\chapter{Experimental setup}
%TODO write something about the machine which ran the experiments and something about how we measure the different things. Maybe copy all this from the AE report?

The experiments were performed on a machine with a Intel i5-3210M @ 2.5GHz (Ivy Bridge) with 128K bytes of L1 cache, 512K bytes of L2 and 3072K bytes of L3 cache. The machine had 4.2GB ram and ran Ubuntu 14.04 with kernel version 3.16.0-50.

The running time was measured using the built in \texttt{high\_resolution\_clock} in the \texttt{chrono} library. This measures the wall clock. It is the clock in \texttt{c++} with the highest precision, i.e. the shortest tick period.

The code is compiled with \texttt{g++ 4.8.4} with the \texttt{c++11} standard enabled and no optimization level.

The elements in our data structures were 32 bit integers. Random elements were generated uniformly in the 32 bit integer range using the Mersenne Twister 19937 from the \texttt{random} library.

CPU measurements were collected using \texttt{PAPI 5.3.0.0}

We used \texttt{perf} to measure page faults.


\chapter{Worst case time bounds of the heaps}
In this section we will argue about how the algorithms perform in the worst case as opposed to the amortized time bounds. We will for each operations on the heaps argue how the worst cases are obtained.

\subsection{FindMin}
Finding the minimum element is in both the Fibonacci heap and the binary heap a constant time operation. We keep a pointer to the minimum element in the Fibonacci heap and in the binary heap we just return the first element in the array which holds the heap.

\subsection{Insert}
Inserting into the binary heap is simple. We insert the element at the last position in the binary tree, i.e. the heap, and bubble the element up to its correct position. In the worst case we need to bubble the element to the top, which means bubbling through $\mathcal{O}(\log n)$ layers which gives a logarithmic complexity in the worst case for this operation.

Inserting in the Fibonacci heap is even simpler. We simply just add the element to the root list which just involves manipulating a doubly linked list, i.e. some pointer calculations. 
%This constant time operation is what allows us to give the amortized analysis of decrease key?

\subsection{DeleteMin}
Delete the minimum element is $\mathcal{O}(\log{n})$ for the binary heap and $\mathcal{O}(\log{n})$ amortized for the Fibonacci heap. This upper bound is trivially obtained in the binary heap by having the heap ordered as a sorted array. This way, whenever we delete the minimum we would move the largest element to the top and bubble it $\log n$ levels down again to the bottom.

It can however be extremely expensive to delete the minimum in the Fibonacci heap. When we delete the minimum from a Fibonacci heap we need to consolidate the heap. This operations is linear in the size of the root list. The root list can grow linearly in the worst case if we do $n$ inserts in a row, then the root list will contain at least $n$ nodes, which needs to be consolidated in time $\mathcal{O}(\vert root\_list \vert)$.

\subsection{DecreaseKey}
Decreasing a key in the binary heap is done by giving the element a new priority and then bubble it up to its new position. The worst case is that the element decreased is the last element and its new priority is the lowest in the tree, which means that it needs to bubble to the top of the heap. This costs $\mathcal{O}(\log n)$.

The worst case for the Fibonacci heap would be if we have a tree of height $\mathcal{O}(\log n)$ where every internal node on the path to the lowest leaf, which is decreased, is marked. This will cause a chain of cuts and cascading cuts all the way up to the root, which will take $\mathcal{O}(\log n)$ time.

\chapter{Heap experiments}
In this section we will design and perform experiments where we try to measure the running time and number of comparisons of the operations for both our priority queue implementations. We will give a summary of the design of each experiment, results, and discussion of the found results for each operation.

\subsection{FindMin}
Since finding the minimum element of both queues are constant time operations and basically just dereferencing a pointer we did not think it made sense to compare the running times in terms of wall clock. What we did instead is we measured the number of cycles for performing a single FindMin operation and the number of conditional branching operations needed. We would highly expect that the results do not depend on the size of the heap, why we made an experiment where the size of the heap was powers of two in the range from $\left[2^0, 2^{22}\right]$. The results can be found in figure~\ref{fig:findmin_cycles} and figure~\ref{fig:findmin_cr}.
Our hypothesis was fortunately true and we do not think that there is much to discuss about the results.

\subsection{Insert}
Inserting in the Fibonacci heap is a constant time operation while inserting into the binary heap is logarithmic. We would like to see that the binary heap performs worse. We will test both priority queues in both the average case and the worst case. In the average case we select a random priority in the integer range and insert it into the heap.
%TODO how high can we expect the elemen to bubble on average?
In the worst case we insert elements into the heap in reverse order. This will cause the elements to bubble all the way to the top on every insert.

We expect to see that the Fibonacci heap is constant time in any case and that the binary heap is logarithmic in both cases. The number of comparisons performed in the Fibonacci heap should therefore also be constant on each insert while the number of comparisons should increase logarithmically in the size of the input.

The results can be found in figure~\ref{fig:insert_t},~\ref{fig:insert_c}.

%TODO Add discussion of the results here!


\subsection{DeleteMin}
Deleting the minimum element takes logarithmic time both priority queues. It can however take linear time in the worst case for the Fibonacci queue if a delete minimum operation is performed right after $n$ insert operations. We measure deleting the minimum in the average case where we randomly insert $n$ elements and then first pop one element from both queues such that the Fibonacci queue has consolidated. We then measure the on deleting one element for different heap sizes. We expect to see both have a logarithmic running time and comparisons.
Testing in the worst case we will do the same but not consolidate the Fibonacci heap first and the binary heap should be inserted in sorted order.

The results can be found in figure~\ref{fig:delmin_c} and \ref{fig:delmin_t}.

%TODO add discussion of results.


%TODO compiling using O2 makes bh better than fh but not in worst case MSP

\chapter{Dijkstra's algorithm}
In this section we introduce families of graphs that we believe makes Dijkstra's algorithm perform many respectively few \texttt{DecreaseKey} operations. It is obvious that the number of \texttt{DecreaseKey} operations is an exact measure of the number of edges we have to relax.

\section{Few relaxations}
Since Dijkstra's algorithm finds shortest paths from the source $s$ to all vertices in the graph, we have to relax at least one ingoing edge for all other vertices. We therefore conclude that the lower bound for relaxing edges must be $\vert V \vert -1$. If not, the graph would contain unvisited vertices. Having a chain of vertices with \textit{weight} 0 on all edges causes Dijkstra to \textit{relax} each edge exactly once, giving us a graph with the desired property. It is clear that not additional (positive weighted) edges will be relaxed. For a graphical representation of the described class of graphs, please to figure~\ref{figure:graph_chain}.

\begin{figure}
\centering
\includegraphics[scale=1]{../figures/graph_chain.png}
\caption{Chain of $\vert V \vert$ nodes with edge weight 0.}
\label{figure:graph_chain}
\end{figure}

\section{Many relaxations}
We consider an analysis of Dijkstra's algorithm using the fact that our implementation makes use of a priority queue $Q$. Since each vertex is removed from $Q$ exactly once, and the adjacency list of each vertex is scanned exactly once, it is clear that we can at most relax all edges exactly once.
In other words we need to consider graphs that makes Dijkstra's algorithm perform $\vert E \vert$ \texttt{DecreaseKey} operations in order to reach the upper bound.

We present two candidates of classes of graphs with the above mentioned property. The first proposal makes use of negative weights. Since we are not introducing negative cycles, Dijkstra's algorithm would still be able run, but as we allow for previously marked nodes to be reinserted into the priority queue, we cannot rely on the asymptotic time bound. The second proposal uses only positive edge weights, and can therefore be compared against the asymptotic analysis.

\begin{figure}
\centering
\includegraphics[scale=0.8]{../figures/graph_neg_weights.png}
\caption{}
\label{figure:graph_neg_weights}
\end{figure}

\begin{figure}
\centering
\centerline {
\includegraphics[scale=0.5]{../figures/graph_positive_weights.png}
}
\caption{}
\label{figure:graph_neg_weights}
\end{figure}

\chapter{Experiments on Dijkstra's}
Perform experiments on your implementation of Dijkstra's algorithm - in particular, try to observe if the version using Fibonacci heaps achieves an improved performance because of the amortized O(1) time DecreaseKey operation.

\section{System configuration and methodology}
\label{sec:machine}
The experiments were performed on a machine with a Intel i5 @ 2.5GHz (Ivy Bridge) with 128K bytes of L1 cache, 512K bytes of L2 and 3072K bytes of L3 cache. The machine had 4.2GB ram and ran Ubuntu 14.04 with kernel version 3.16.0-30. The internal mechanical disc was a 1TB Samsung SpinPoint M8 SATA II 2.5'' laptop hard drive with 8MB cache running at 5400 RPM with a seek time of 12.0ms.

The running time was measured using the built in \texttt{high\_resolution\_clock} in the \texttt{chrono} library. This measures the wall clock. It is the clock in \texttt{c++} with the highest precision, i.e. the shortest tick period.

The code is compiled with \texttt{g++ 4.8.2} with the \texttt{c++11} standard enabled and no optimization level.

Correctness was tested on random 32-bit integers generated uniformly in the integer range [0,...,1000] using the Mersenne Twister 19937 from the \texttt{random} library.

Running time was on random 32-bit integers generated using the system call \textit{head -c 64GB $<$dev/urandom $>$test64gb}.

All experiments were done using \textit{bstream} with a buffer size of 2048 integers (8192 bytes).

Each experiment was executed 5 times and the average was used as the result.


\bibliography{references}

\end{document}


